# 期中项目选题

> **项目形式**：小组合作（3-5人/组）
> **提交截止**：期中周

---

## 目录

1. [选题概览](#选题概览)
2. [选题一：迷宫探索与自主导航](#选题一迷宫探索与自主导航)
3. [选题二：彩色物体追踪与定位](#选题二彩色物体追踪与定位)
4. [选题三：目标搜索与巡逻任务](#选题三目标搜索与巡逻任务)
5. [选题四：多传感器融合的动态避障](#选题四多传感器融合的动态避障)
6. [项目报告要求](#项目报告要求)
7. [提交清单](#提交清单)

---

## 选题概览

|  选题  |  难度  |      核心能力      |   适合团队   |
| :--: | :--: | :------------: | :------: |
| 迷宫探索 |  ⭐⭐⭐ |   路径规划 + 激光雷达  |  对算法感兴趣  |
| 物体追踪 |  ⭐⭐⭐ |   视觉感知 + 坐标变换  | 对视觉处理感兴趣 |
| 目标搜索 | ⭐⭐⭐⭐ |  规划 + 感知 + 状态机 |  想挑战综合系统 |
| 动态避障 | ⭐⭐⭐⭐ | 多传感器融合 + 反应式控制 | 想深入传感器技术 |

---

## 选题一：迷宫探索与自主导航

### 1. 题目描述

机器人被放置在一个未知迷宫的入口处，需要**自主探索**并找到通往出口的路径。机器人仅能使用激光雷达感知周围环境，不能提前获取迷宫地图。

**任务目标**：

* 从起点出发，成功到达迷宫终点
* 全程自主运行，无人工干预
* 避免与墙壁发生碰撞

### 2. 演示效果

```
┌─────────────────────────────────┐
│  ████████████████████████████   │
│  █ 🤖→  █        █          █   │
│  █ ███  █  ████  █  ██████  █   │
│  █      █     █     █       █   │
│  ██████████   █  ████  ██████   │
│  █            █        █    🏁  │
│  ████████████████████████████   │
└─────────────────────────────────┘
       🤖 起点    🏁 终点
```

**预期展示**：

* 机器人在迷宫中自主移动
* RViz 中实时显示激光雷达扫描数据
* 机器人不与墙壁碰撞，最终到达终点

### 3. 推荐技术栈

| 模块       | 技术/话题                                | 说明       |
| -------- | ------------------------------------ | -------- |
| **感知**   | `/scan` (sensor_msgs/LaserScan)      | 获取激光雷达数据 |
| **控制**   | `/cmd_vel` (geometry_msgs/Twist)     | 发布速度指令   |
| **可视化**  | RViz + LaserScan Display             | 查看雷达数据   |
| **算法参考** | Wall Following / Bug Algorithm / DFS | 选择一种实现   |

**核心代码结构建议**：

```
maze_explorer/
├── scripts/
│   ├── explorer_node.py      # 主控制节点
│   └── laser_processor.py    # 激光数据处理
├── launch/
│   └── maze_explorer.launch
└── CMakeLists.txt & package.xml
```

### 4. 加分项

| 加分项       | 分值   | 描述                |
| --------- | ---- | ----------------- |
| 🌟 路径可视化  | +10% | 在 RViz 中绘制机器人行走轨迹 |
| 🌟 多种算法对比 | +10% | 实现并对比 2 种以上导航算法   |
| 🌟 死胡同处理  | +5%  | 优雅处理死胡同回退逻辑       |
| 🌟 最短路径优化 | +10% | 在多次运行后学习更优路径      |

---

## 选题二：彩色物体追踪与定位

### 1. 题目描述

场景中放置了若干**彩色方块**（如绿色、红色、蓝色）。机器人需要使用相机识别**指定颜色**的目标，计算其**三维坐标**，并**自主导航**至目标前方停下。

**任务目标**：

* 正确识别指定颜色的物体
* 计算目标在机器人坐标系下的 3D 位置
* 驱动机器人移动至目标前方（距离 0.3m 内）

### 2. 演示效果

```
        📷 机器人视野
   ┌─────────────────────┐
   │                     │
   │    🟥      🟩       │  ← 检测到绿色方块
   │           ┌──┐      │
   │           │✓ │      │  ← 标记检测框
   │           └──┘      │
   │                     │
   └─────────────────────┘
   
   终端输出：
   [INFO] 检测到目标: 绿色方块
   [INFO] 相机坐标: (0.82, 0.15, 1.20)
   [INFO] 机器人坐标: (1.20, 0.15, 0.82)
   [INFO] 正在接近目标...
```

**预期展示**：

* 图像窗口中显示检测框和目标中心点
* 终端实时输出目标的 3D 坐标
* 机器人平稳移动至目标前方停止

### 3. 推荐技术栈

| 模块         | 技术/话题                            | 说明         |
| ---------- | -------------------------------- | ---------- |
| **RGB 图像** | `/camera/rgb/image_raw`          | 颜色检测       |
| **深度图像**   | `/camera/depth/image_raw`        | 获取深度信息     |
| **相机内参**   | `/camera/rgb/camera_info`        | 像素→3D 坐标转换 |
| **坐标变换**   | TF (`camera_link` → `base_link`) | 坐标系转换      |
| **图像处理**   | OpenCV (HSV + Contour)           | 颜色分割与轮廓检测  |
| **控制**     | `/cmd_vel`                       | 视觉伺服控制     |

**关键公式**：

```
# 像素坐标 (u, v) + 深度 d → 相机坐标系 3D 点
X_cam = (u - cx) * d / fx
Y_cam = (v - cy) * d / fy
Z_cam = d
```

### 4. 加分项

| 加分项             | 分值   | 描述          |
| --------------- | ---- | ----------- |
| 🌟 多目标检测        | +10% | 同时识别多个颜色/物体 |
| 🌟 动态目标追踪       | +15% | 目标移动时持续跟随   |
| 🌟 Open3D 点云可视化 | +10% | 3D 重建目标所在区域 |
| 🌟 HSV 参数自适应    | +5%  | 自动适应光照变化    |

---

## 选题三：目标搜索与巡逻任务

### 1. 题目描述

机器人需要在一个**多区域场景**中执行巡逻任务。场景中随机放置了若干**目标物体**（绿色方块）。机器人需要：

1. **规划巡逻路径**覆盖所有区域
2. **检测并记录**发现的目标位置
3. 巡逻结束后**返回起点**
4. **发布报告**汇总所有发现的目标

### 2. 演示效果

```
   场景俯视图
┌────────────────────────────┐
│  ┌─────┐      ┌─────┐     │
│  │ A 🟩│      │  B  │     │
│  └─────┘      └─────┘     │
│         🤖                 │
│  ┌─────┐      ┌─────┐     │
│  │  C  │      │ D 🟩│     │
│  └─────┘      └─────┘     │
│                       🏠   │
└────────────────────────────┘
   🟩 目标物体  🏠 起点/终点

终端输出：
[INFO] === 巡逻任务开始 ===
[INFO] 前往区域 A...
[INFO] 发现目标! 位置: (2.3, 1.5, 0.1)
[INFO] 前往区域 B...
[INFO] 区域 B 未发现目标
[INFO] 前往区域 C...
[INFO] 前往区域 D...
[INFO] 发现目标! 位置: (4.1, -1.2, 0.1)
[INFO] 返回起点...
[INFO] === 巡逻报告 ===
[INFO] 发现目标数量: 2
[INFO] 目标位置列表:
[INFO]   1. (2.3, 1.5, 0.1)1.5, 0.1)
[INFO]   2. (4.1, -1.2, 0.1)
```

### 3. 推荐技术栈

| 模块       | 技术                  | 说明       |
| -------- | ------------------- | -------- |
| **区域导航** | 预设航点 + `/cmd_vel`   | 依次访问各区域  |
| **目标检测** | RGB + HSV 分割        | 识别绿色方块   |
| **位置记录** | 深度相机 + TF           | 计算目标世界坐标 |
| **状态管理** | 状态机 (State Machine) | 管理巡逻逻辑   |
| **结果发布** | 自定义 ROS Message     | 汇报检测结果   |

**状态机设计参考**：

```
         ┌──────────────┐
         │   PATROLLING │ ◄─────────┐
         └──────┬───────┘           │
                │ 到达区域           │
                ▼                   │
         ┌──────────────┐           │
         │  SEARCHING   │           │
         └──────┬───────┘           │
                │ 完成搜索           │
                ▼                   │
         ┌──────────────┐  还有区域  │
         │   LOGGING    │ ──────────┘
         └──────┬───────┘
                │ 所有区域完成
                ▼
         ┌──────────────┐
         │  RETURNING   │
         └──────┬───────┘
                │ 到达起点
                ▼
         ┌──────────────┐
         │   REPORT     │
         └──────────────┘
```

### 4. 加分项

| 加分项           | 分值   | 描述                 |
| ------------- | ---- | ------------------ |
| 🌟 智能路径优化     | +10% | 根据区域位置优化访问顺序（TSP）  |
| 🌟 避障能力       | +10% | 巡逻途中避开未知障碍物        |
| 🌟 目标分类       | +10% | 区分不同颜色/类型的目标       |
| 🌟 RViz 标记可视化 | +5%  | 用 Marker 标注发现的目标位置 |
| 🌟 区域覆盖率统计    | +5%  | 计算并显示覆盖率           |

---

## 选题四：多传感器融合的动态避障

### 1. 题目描述

机器人需要在一个复杂环境中从起点导航至终点。场景中包含：

* **静态障碍物**：固定位置的墙壁、柱子
* **动态障碍物**：移动的物体（如另一个移动机器人）

机器人需要**融合激光雷达和深度相机**的数据，实时检测障碍物并做出**反应式避障**决策。

### 2. 演示效果

```
  场景示意图
┌─────────────────────────────┐
│ 🏁                         │
│  ▲                         │
│  │         ████                        │
│  │                         │
│  │    →🚗    ████          │  ← 动态障碍物移动中
│  │                         │
│  │              ████       │
│ 🤖                         │
└─────────────────────────────┘

   🤖 机器人  🚗 移动障碍  ████ 静态障碍

终端输出：
[INFO] 目标位置: (5.0, 4.0)
[INFO] 当前位置: (0.5, 0.3)
[WARN] 检测到动态障碍物! 距离: 1.2m 方向: 45°
[INFO] 执行避障机动...
[INFO] 障碍物已通过，恢复导航
[INFO] 已到达目标位置!
```

### 3. 推荐技术栈

| 模块        | 技术                        | 说明         |
| --------- | ------------------------- | ---------- |
| **激光雷达**  | `/scan`                   | 360° 障碍物检测 |
| **深度相机**  | `/camera/depth/image_raw` | 前方精细检测     |
| **传感器融合** | 自定义融合逻辑                   | 综合两种传感器优势  |
| **避障算法**  | VFH / DWA / APF           | 反应式避障策略    |
| **动态检测**  | 帧间差分 / 点云对比               | 识别移动物体     |
| **控制**    | `/cmd_vel`                | 速度控制       |

**传感器融合思路**：

```
激光雷达优势：
  ✓ 360° 全向感知
  ✓ 精确距离测量
  ✗ 无法识别物体类型

深度相机优势：
  ✓ 丰富的空间信息
  ✓ 可结合 RGB 识别物体
  ✗ 视野有限 (前方)

融合策略：
  → 深度相机负责前方精细检测
  → 激光雷达负责侧向和后方感知
  → 动态障碍物优先由相机检测
```

### 4. 加分项

| 加分项         | 分值   | 描述            |
| ----------- | ---- | ------------- |
| 🌟 动态障碍物预测  | +15% | 预测移动障碍物轨迹     |
| 🌟 速度自适应    | +10% | 根据障碍物密度调整速度   |
| 🌟 融合点云可视化  | +10% | Open3D 展示融合结果 |
| 🌟 多动态障碍物处理 | +10% | 同时处理多个移动物体    |
| 🌟 安全距离参数化  | +5%  | 支持动态配置安全距离    |

---

## 项目报告要求

报告应采用 **PDF 格式**，篇幅建议 **10-15 页**（不含代码附录），包含以下内容：

### 📄 报告结构

#### 1. 项目概述（1 页）

* 选题名称与任务描述
* 团队成员信息
* 项目目标

#### 2. 系统设计（2-3 页）

* **系统架构图**：展示各节点及其通信关系
* **模块划分**：各模块功能说明
* **数据流图**：传感器数据如何流经系统

#### 3. 算法原理（2-3 页）

* 核心算法的数学原理或逻辑描述
* 为什么选择该算法？对比其他方案的优劣
* 关键参数说明

#### 4. 实现细节（2-3 页）

* 关键代码片段及注释（非完整代码）
* 遇到的技术难点及解决方案
* 调试过程中的问题记录

#### 5. 实验结果（2-3 页）

* **定量结果**：成功率、耗时、精度等数据
* **定性展示**：截图、轨迹图、可视化结果
* **结果分析**：为什么好/不好？如何改进？

#### 6. 团队分工与贡献（1 页）

> ⚠️ **此部分为必填项，需全体成员确认签字**

| 姓名 | 学号      | 负责模块 | 具体工作内容      | 贡献占比 | 组员互评  |
| -- | ------- | ---- | ----------- | ---- | ----- |
| 张三 | 20xx... | 感知模块 | HSV 调参、深度融合 | 30%  | 4.5/5 |
| 李四 | 20xx... | 规划模块 | A* 算法实现     | 25%  | 4.0/5 |
| 王五 | 20xx... | 控制模块 | 视觉伺服、速度平滑   | 25%  | 4.5/5 |
| 赵六 | 20xx... | 集成测试 | 系统调试、视频录制   | 20%  | 4.0/5 |

> **注意**：贡献占比总和必须为 **100%**

#### 7. 总结与反思（0.5-1 页）

* 项目收获
* 不足与改进方向
* 对课程的建议（可选）

#### 附录（可选）

* 完整代码仓库链接
* 额外的实验数据
* 参考资料列表

---

## 提交清单

请在截止日期前提交以下材料：

|  序号 | 提交物          | 格式                | 要求            |
| :-: | ------------ | ----------------- | ------------- |
|  1  | 📦 源代码       | ROS Package (zip) | 可编译、可运行       |
|  2  | 📹 演示视频      | MP4 (≤ 100MB)     | 3-5 分钟，展示完整功能 |
|  3  | 📄 项目报告      | PDF               | 按上述模板撰写       |
|  4  | 💾 rosbag 数据 | .bag 文件           | 记录一次完整运行      |

### 视频要求

* **时长**：3-5 分钟
* **内容**：

  1. 简要介绍项目目标（30 秒）
  2. 展示 Gazebo 仿真运行效果（2-3 分钟）
  3. 展示 RViz 可视化效果（1 分钟）
  4. 简要说明关键实现（30 秒）
* **格式**：MP4，分辨率 ≥ 720p

---

## 💡 温馨提示

1. **尽早开始**：不要等到最后一周才动手
2. **善用 Demo**：观看助教的演示视频理解目标效果
3. **多查文档**：ROS Wiki、OpenCV 文档是最好的老师
4. **版本控制**：强烈建议使用 Git 管理代码
5. **及时提问**：遇到问题先搜索，解决不了及时联系助教

---

> **祝大家项目顺利！** 🚀
